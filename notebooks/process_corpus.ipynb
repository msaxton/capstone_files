{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a collection of documents into a  Gensim corpus for topic modeling\n",
    "\n",
    "The work of constructing a topic model is split between the machine and the human user; although most of the computation is done by the machine, there is still a great deal of things that the human user needs to do. Perhaps the most important task for the human user is prepare a collection of documents so that the machine can use it to build the topic model. \n",
    "\n",
    "In this notebook, I prepare a collection of documents, namely articles from the *Journal of Biblical Literature* (*JBL*), into a format that will be most useful for the machine to build the topic model. When the process is completed, there will be a dictionary which maps the articles (documents) to the appropriate metadata and two corpora: the first a general corpus in which each journal article is represented by a list of informative words (minus pre-defined stop words), and second a corpus in which each journal article is represented by a list of the most informative *nouns* only."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools \n",
    "There are several python libraries which I use to process the *JBL* into a corpus ready to be modeled. The most important of which are:\n",
    "* `spacy`, a natural language processing library which allows me to prepare the text in meaningful was such as removing stopwords, part of speech tagging, and removing stop words\n",
    "* `gensim`, a topic modeling library  which does the computational work of building the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Note, this code last ran 24 April 2018 7:50pm\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import xml.etree.ElementTree as ET\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up preprocessing\n",
    "The output of a topic model will only be as good as its input. It is important therefore to select the most informative words, or features, from the corpus. This allows the topic model to focus in on the \"signal\" by eliminating as much \"noise\" as possible. Toward that end, I take a few steps:\n",
    "* I identify a number of common abbreviations used in *JBL* and map them to their longer forms. If the model counts 'gen' and 'genesis' as two unique tokens, it may skew the results of the model.\n",
    "* I remove a number of tokens which are not informative features. These may be tokens that result from ocr inaccuracies, they may be roman numerals (used often in this corpus), they may be transliterated Greek or Hebrew stopwords, or they may be the names of cities and publishers referred to in the corpus. Whatever the case may be they distract from more informative content words.\n",
    "* I also remove both English and German stopwords from the corpus. The latter is necessary because there are a number of German quotations even in the English language documents.\n",
    "\n",
    "Preprocessing also includes the process of tokenizing each document, that is to say, transforming each document to a list of discrete items. In this case, each item is a word. In the functions I define below for this purpose, for each document I create a list of general tokens (minus stopwords) and a list of noun only tokens. This allows me to build to different versions of the topic model which I compare to see which of the models is most informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_abbrs = [('gen', 'genesis'),('exod', 'exodus'),('ex', 'exodus'),('lev', 'leviticus'),('num', 'numbers'),\n",
    "               ('deut', 'deuteronomy'),('josh', 'joshua'),('judg', 'judges'), ('jud', 'judges'),('sam', 'samuel'),('kgs', 'kings'),\n",
    "               ('chr', 'chronicles'),('neh', 'nehemiah'),('esth', 'esther'),('ps', 'psalms'),('pss', 'psalms'),\n",
    "               ('prov', 'proverbs'),('eccl', 'ecclesiastes'),('qoh', 'qoheleth'), ('isa', 'isaiah'),\n",
    "               ('jer', 'jeremiah'),('lam', 'lamentations'),('ezek', 'ezekiel'),('hos', 'hosea'),('obad', 'obediah'),\n",
    "               ('mic', 'micah'),('nah', 'nahum'),('hab', 'habakkuk'),('zeph', 'zephaniah'),('hag', 'haggai'),\n",
    "               ('zech', 'zechariah'),('mal', 'malachi'),('matt', 'matthew'),('mk', 'mark'),('lk', 'luke'),\n",
    "               ('jn', 'john'),('rom', 'romans'),('cor', 'corinthians'),('gal', 'galatians'),('eph', 'ephesians'),\n",
    "               ('phil', 'philippians'),('col', 'colossians'),('thess', 'thessalonians'),('tim','timothy'),\n",
    "               ('phlm', 'philemon'),('heb', 'hebrews'),('jas', 'james'),('pet', 'peter'),('rev', 'revelation'),\n",
    "               ('tob', 'tobit'),('jdt', 'judith'), ('wis', 'wisdom of solomon'),('sir', 'sirach'), ('bar', 'baruch'),\n",
    "               ('macc', 'maccabees'), ('esd', 'esdras'), ('tg', 'targum')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = ['ab', 'al', 'alten', 'america', 'atlanta', 'au', 'av', 'avrov', 'b', 'ba', 'bauer', 'berlin',\n",
    "                    'boston', 'brill', 'brown', 'c', 'cad', 'cambridge', 'cf', 'ch', 'chap', 'chapter', 'charles',\n",
    "                    'chicago', 'chs', 'cit', 'cite', 'claremont', 'college', 'craig', 'cum', 'd', 'dans', 'dennis',\n",
    "                    'diese', 'dissertation', 'dm', 'dtr', 'ed', 'eds', 'eerdmans', 'ek', 'elisabeth', 'en', 'et',\n",
    "                    'ev', 'ez', 'f', 'far', 'ff', 'fiir', 'g', 'gar', 'george', 'geschichte', 'gott', 'gottes',\n",
    "                    'grand', 'h', 'ha', 'hall', 'hartford', 'hat', 'haven', 'henry', 'ia', 'ibid', 'io',\n",
    "                    'isbn', 'iv', 'ivye', 'ix', 'jeremias', 'jesu', 'k', 'ka', 'kai', 'kal', 'kat', 'kee', 'ki', 'kim',\n",
    "                    'kirche', 'klein', 'knox', 'l', 'la', 'le', 'leiden', 'leipzig', 'les', 'loc', 'louisville', 'm',\n",
    "                    'ma', 'madison', 'marie', 'marshall', 'mohr', 'n', 'na', 'neuen', 'ni', 'nu', 'nur', 'o', 'ol',\n",
    "                    'om', 'op', 'ov', 'ovadd', 'ovk', 'oxford', 'paper', 'paulus', 'ph', 'philadelphia', 'post',\n",
    "                    'pres', 'president', 'press', 'pro', 'prof', 'professor', 'r', 'ra', 'rab', 'rapids', 'refer',\n",
    "                    'reviews', 'ro', 'robert', 'robinson', 'rov', 's', 'sa', 'schmidt', 'schriften', 'scott', 'sec',\n",
    "                    'section', 'seiner', 'sheffield', 'siebeck', 'stanely', 'studien', 't', 'thee', 'theologie',\n",
    "                    'they', 'thing', 'thou', 'thy', 'tiibingen', 'tov', 'tr', 'tv', 'u', 'um', 'univ', 'unto', 'v',\n",
    "                    'van', 'verse', 'vol', 'volume', 'vs', 'vss', 'vv', 'w', 'william', 'wunt',\n",
    "                    'y', 'yap', 'ye', 'york', 'zeit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../romannumeral.txt') as f:\n",
    "    rom_nums = f.read()\n",
    "rom_nums = re.sub('romannumeral', '', rom_nums)\n",
    "rom_nums = re.sub('lxx', '', rom_nums) # lxx is an abbr. for 'septuagint'\n",
    "rom_nums = re.split(r'\\t\\n', rom_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/german_stop_words') as f:\n",
    "    german_stop_words = f.readlines()\n",
    "    german_stop_words = [word.strip() for word in german_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "stop_words = spacy.en.STOPWORDS\n",
    "stop_words.update(custom_stop_words)\n",
    "stop_words.update(rom_nums)\n",
    "stop_words.update(german_stop_words)\n",
    "\n",
    "\n",
    "def substitute(list_tuples, string):\n",
    "    for tuple_ in list_tuples:\n",
    "        string = re.sub(r'\\b' + tuple_[0] + r'\\b', tuple_[1], string)\n",
    "    return string\n",
    "\n",
    "def get_lemmas(doc):\n",
    "    tokens = [token for token in doc]\n",
    "    lemmas = [token.lemma_ for token in tokens if token.is_alpha]\n",
    "    lemmas = [lemma for lemma in lemmas if lemma not in stop_words]\n",
    "    for index, item in enumerate(lemmas):\n",
    "        item = substitute(books_abbrs, item)\n",
    "        lemmas[index] = item\n",
    "    return lemmas\n",
    "\n",
    "def get_noun_lemmas(doc):\n",
    "    tokens = [token for token in doc]\n",
    "    noun_tokens = [token for token in tokens if token.tag_ == 'NN' or token.tag_ == 'NNP' or token.tag_ == 'NNS']\n",
    "    noun_lemmas = [noun_token.lemma_ for noun_token in noun_tokens if noun_token.is_alpha]\n",
    "    noun_lemmas = [noun_lemma for noun_lemma in noun_lemmas if noun_lemma not in stop_words]\n",
    "    for index, item in enumerate(noun_lemmas):\n",
    "        item = substitute(books_abbrs, item)\n",
    "        noun_lemmas[index] = item\n",
    "    return noun_lemmas\n",
    "    \n",
    "def process_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = get_lemmas(doc)\n",
    "    noun_lemmas = get_noun_lemmas(doc)\n",
    "    return lemmas, noun_lemmas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and map metadata to text, implement preprocessing\n",
    "JSTOR's Data for Research provided both metadata and full text articles for *JBL* in the form of xml files and txt files respectively. However, many of these articles are not useful for the topic model and are not processed:\n",
    "\n",
    "* Many articles in this collection are written in German. I have decided to eliminate them because a multilingual corpus is less likely to produce an informative model.\n",
    "* Other \"articles\" in the corpus are not articles at all, but are instead \"front/back matter\", \"annual indices\", or \"volume information.\" The text in these articles are not informative for the purposes of this topic model.\n",
    "\n",
    "After, leaving those articles aside, I extract the metadata from the xml files, map the metadata to the relevant article, and then store this mapping as a dictionary for later reference. Then, I extract the full text for each article and run it through the preprocessing functions I defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finhsied doc  1000\n",
      "finhsied doc  2000\n",
      "finhsied doc  3000\n",
      "finhsied doc  4000\n",
      "finhsied doc  5000\n",
      "finhsied doc  6000\n",
      "finhsied doc  7000\n",
      "finhsied doc  8000\n",
      "finhsied doc  9000\n",
      "CPU times: user 1h 8min 18s, sys: 43.1 s, total: 1h 9min 1s\n",
      "Wall time: 1h 15min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xml_files = sorted(os.listdir('../data/metadata/'))\n",
    "txt_files = sorted(os.listdir('../data/ocr/'))\n",
    "mapping_dict = collections.OrderedDict()\n",
    "general_docs = []\n",
    "noun_docs = []\n",
    "i = 0\n",
    "for xml, txt in zip(sorted(xml_files), sorted(txt_files)):\n",
    "    article_dict = {}\n",
    "    # read xml file\n",
    "    tree = ET.parse('../data/metadata/' + xml)\n",
    "    root = tree.getroot()\n",
    "    # only process english articles\n",
    "    lang = root.find('./front/article-meta/custom-meta-group/custom-meta/meta-value')\n",
    "    if (lang.text == 'eng') or (lang.text == 'en'):\n",
    "        # add title to article dict\n",
    "        title = root.find('./front/article-meta/title-group/article-title')\n",
    "        try:\n",
    "            title = title.text\n",
    "            title = title.lower()\n",
    "        except AttributeError:\n",
    "            book_reviewed = root.find('./front/article-meta/product/source')\n",
    "            title = 'review of ' + book_reviewed.text.lower() # jbl does not title book reviews\n",
    "        unwanted_titles = ['front matter', 'back matter', 'annual index', 'volume information'] # ignore these titles\n",
    "        if not title in unwanted_titles:\n",
    "            article_dict['title'] = title\n",
    "            # add article_id to article_dict\n",
    "            article_id = root.find('./front/article-meta/article-id')\n",
    "            article_id = article_id.text.lower()\n",
    "            article_dict['article_id'] = article_id\n",
    "            # add author to article_dict\n",
    "            f_name = root.find('./front/article-meta/contrib-group/contrib/string-name/given-names')\n",
    "            l_name = root.find('./front/article-meta/contrib-group/contrib/string-name/surname')\n",
    "            author = root.find('./front/article-meta/contrib-group/contrib/string-name')\n",
    "            if f_name != None:\n",
    "                author = l_name.text + ', ' + f_name.text\n",
    "            elif author != None:\n",
    "                author = root.find('./front/article-meta/contrib-group/contrib/string-name')\n",
    "                author = author.text\n",
    "            else:\n",
    "                author = 'author not listed'\n",
    "            article_dict['author'] = author\n",
    "            # add publish date to article_dict\n",
    "            pub_year = root.find('./front/article-meta/pub-date/year')\n",
    "            article_dict['pub_year'] = pub_year.text\n",
    "            key = 'doc_' + str(i)\n",
    "            mapping_dict[key] = article_dict\n",
    "            \n",
    "            # read txt file\n",
    "            with open('../data/ocr/' + txt, mode='r', encoding='utf8') as f:\n",
    "                text = f.read()\n",
    "            lemmas, nouns = process_text(text)\n",
    "            if len(nouns) > 0: # only want docs which are not empty\n",
    "                general_docs.append(lemmas)\n",
    "                noun_docs.append(nouns)\n",
    "                key = 'doc_' + str(i)\n",
    "                mapping_dict[key] = article_dict\n",
    "                i += 1\n",
    "            else:\n",
    "                continue\n",
    "            if i % 1000 == 0:  # displaying progress\n",
    "                print('finhsied doc ', i)\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/doc2metadata.json', encoding='utf8', mode='w') as outfile:\n",
    "    json.dump(mapping_dict, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Gensim corpora\n",
    "Initializing a Gensim corpus (which serves as the basis of a topic model) entails two steps:\n",
    "1. Creating a dictionary which contains the list of unique tokens in the corpus mapped to an integer id.\n",
    "2. Initializing the corpus on the basis of the dictionary just created. Each document in a Gensim corpus is a list of tuples. The first element of the tuple is an integer id for each unique word type and the second tuple is a count of how often that unique word type occurs in the document. So, the first five elements of a document may look something like this: `[(1, 1.0), (2, 1.0), (3, 5.0), (4, 1.0), (5, 9.0)]` \n",
    "\n",
    "The creation of the dictionary requires the human user to specify two important parameters which further identify the most informative features.\n",
    "* `no_below` This parameter filters out words which are too rare to be informative. The value of this parameter indicates the number of documents in which a token appears. Here I have set the value to 100 indicating that if a token occurs in less than 100 documents, it will not be included in the topic model.\n",
    "* `no_above` This parameter filters out words which are too frequent to be informative The value of this parameter indicates the percentage of documents in which a token appears. Here, I have set the value to 0.5 indicating that if a token occurs in more than 50% of the documents, it will not be included in the topic model.\n",
    "\n",
    "This filtering selects a \"goldilocks zone\" of informative features. Tokens which are too rare fail to register similarity among documents, because so few documents use them. By contrast, words which are too frequent fail to register difference among documents because so many documents share them in common.\n",
    "\n",
    "There is no formula for deciding what counts as too rare or too frequent. It just depends on the size of the corpus and its lexical diversity. I tried a number of variations for these parameters. Frist, taking `no_above` as a constant:\n",
    "* `no_below=20` and `no_above=0.5` which left 22,283 unique tokens\n",
    "* `no_below=50` and `no_above=0.5` which left 12,642 unique tokens\n",
    "* `no_below=100` and `no_above=0.5` which left 7,834 unique tokens\n",
    "\n",
    "Then taking `no_below` as a constant:\n",
    "* `no_below=100` and `no_above=0.3` which left 7,617 unique tokens\n",
    "* `no_below=100` and `no_above=0.4` which left 7,753 unique tokens\n",
    "* `no_below=100` and `no_above=0.5` which left 7,834 unique tokens\n",
    "* `no_below=100` and `no_above=0.9` which left 7,901 unique tokens\n",
    "\n",
    "As you can see, adjusting `no_below` had a greater effect on the number of unique tokens than did adjusting `no_above`. The model I tested with `no_below=20` and `no_above=0.5` contained more \"junk topics\" than did other models so I decided 22,283 unique tokens was too many feautres for this corpus. `no_below=50` and `no_above=0.5` rendred fewer junk topics, but `no_below=100` and `no_above=0.5` did even better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Corpus\n",
    "This corpus contains lemmatized forms of the words used in the text (minus the stopwords outlined above) regardless of their part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "general_dictionary = corpora.Dictionary(general_docs)\n",
    "general_dictionary.filter_extremes(no_below=100, no_above=0.5)\n",
    "general_dictionary.save('../general_corpus/general_corpus.dict')\n",
    "\n",
    "# create corpus\n",
    "corpus = [general_dictionary.doc2bow(doc) for doc in general_docs]\n",
    "corpora.MmCorpus.serialize('../general_corpus/general_corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noun Corpus\n",
    "This corpus contains lemmatized forms of the *nouns* used in the text (minus the stop words outlined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "noun_dictionary = corpora.Dictionary(noun_docs)\n",
    "noun_dictionary.filter_extremes(no_below=100, no_above=0.5)\n",
    "noun_dictionary.save('../noun_corpus/noun_corpus.dict')\n",
    "\n",
    "# create corpus\n",
    "noun_corpus = [noun_dictionary.doc2bow(doc) for doc in noun_docs]\n",
    "corpora.MmCorpus.serialize('../noun_corpus/noun_corpus.mm', noun_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
